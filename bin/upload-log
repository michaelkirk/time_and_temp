#!/usr/bin/env python


file_path = "/home/timeandtemp/logs/time_and_temp.log"
bucket = "altspace-time-and-temp"


import logging
import boto3
from botocore.exceptions import ClientError
import os
import io

# failed attempt with tinys3
# import tinys3
# 
# 
# conn = tinys3.Connection(S3_ACCESS_KEY, S3_SECRET_KEY, tls=True)
# 
# f = open(file_path, 'rb')
# conn.upload('time_and_temp.log', f, bucket)

def upload_public_file(file_name, bucket, object_name=None):
    """Upload a file to an S3 bucket

    :param file_name: File to upload
    :param bucket: Bucket to upload to
    :param object_name: S3 object name. If not specified then file_name is used
    :return: True if file was uploaded, else False
    """

    # If S3 object_name was not specified, use file_name
    if object_name is None:
        object_name = os.path.basename(file_name)

    # Upload the file
    session = boto3.Session(
        aws_access_key_id=os.environ['S3_ACCESS_KEY'],
        aws_secret_access_key=os.environ['S3_SECRET_KEY'],
    )

    print("bucket: {}, file_name: {}, object: {}".format(bucket, file_name, object_name))
    s3 = session.resource('s3')
    try:
        metadata = {'ACL':'public-read', 'ContentEncoding': 'gzip'}
        response = s3.Bucket(bucket).upload_file(file_name, object_name, ExtraArgs=metadata)
    except ClientError as e:
        logging.error(e)
        return False
    return True

def upload_last_line_to_public_file(file_name, bucket, object_name):
    # Upload the file
    session = boto3.Session(
        aws_access_key_id=os.environ['S3_ACCESS_KEY'],
        aws_secret_access_key=os.environ['S3_SECRET_KEY'],
    )

    with open(file_name, 'rb') as f:
        try:  # catch OSError in case of a one line file
            f.seek(-2, os.SEEK_END)
            while f.read(1) != b'\n':
                f.seek(-2, os.SEEK_CUR)
        except OSError:
            f.seek(0)
        last_line = f.readline().decode()

    print("bucket: {}, file_name: {}, object: {}, last_line: {}".format(bucket, file_name, object_name, last_line))

    s3 = session.resource('s3')
    try:
        content = io.BytesIO(last_line.encode("utf-8"))
        metadata = ExtraArgs={'ACL':'public-read'}
        response = s3.Bucket(bucket).upload_fileobj(content, object_name, ExtraArgs=metadata)
    except ClientError as e:
        logging.error(e)
        return False
    return True


def load_env_file(dotenv_path, override=False):
    with open(dotenv_path) as file_obj:
        lines = file_obj.read().splitlines()  # Removes \n from lines

    dotenv_vars = {}
    for line in lines:
        line = line.strip()
        if not line or line.startswith("#") or "=" not in line:
            continue

        key, value = line.split("=", maxsplit=1)
        dotenv_vars.setdefault(key, value)

    if override:
        os.environ.update(dotenv_vars)
    else:
        for key, value in dotenv_vars.items():
            os.environ.setdefault(key, value)

def compress_file(input_path):
    import subprocess
    process = subprocess.Popen(['gzip', '-k', '-f', '-9', input_path])
    _stdout, _stderr = process.communicate()
    return input_path + '.gz'

if __name__ == '__main__':
    import os.path

    dir = os.path.dirname(os.path.realpath(__file__))
    env_path = os.path.join(dir, ".env")

    load_env_file(env_path)

    most_recent_name = "most_recent_" + os.path.basename(file_path)
    upload_last_line_to_public_file(file_path, bucket, most_recent_name)

    compressed_file_path = compress_file(file_path)
    upload_public_file(compressed_file_path, bucket, os.path.basename(file_path))


